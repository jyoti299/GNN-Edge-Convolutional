import onnx
import onnxruntime as ort
"""
Exporting a model from pytorch to onnx and rnunning it using onnx runtime.
"""

torch.manual_seed(0)

def to_numpy(tensor):
    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()

#trainLoader = DataLoader(trainDataset, batch_size=1)

# device = torch.device('cpu')
# print(">>> Loading model weights.")
# model = MLP_TracksterLinkingNet(input_dim = trainDataset[0].x.shape[1])
# modelFolder = "./model_saved/mlp_simple_double_pion_0_PU_simple_training_all_29_features_normalized/"
# checkpoint = torch.load(modelFolder + 'model.pt', map_location=device)
# model.load_state_dict(checkpoint["model_state_dict"])
# print(f"Model epoch: {checkpoint['epoch']}")
# print(">>> Model Loaded.")
# model.to(device)
# model = model.cpu()
# model.eval()

model = GNN_TrackLinkingNet(input_dim=19, hidden_dim=16, output_dim=1, niters=2, dropout=0.2,
                 edge_feature_dim=12, edge_hidden_dim=16, weighted_aggr=True)
device = torch.device('cpu')
model.to(device)
model = model.cpu()
model.eval()


# create dummy input for the model
data = next(iter(train_dl)).to(device)
dummy_input = model.prepare_network_input_data(data.x, data.edge_index, data.edge_features)
pred = model(*dummy_input)
    
print(pred.shape)
print(dummy_input[0].shape, dummy_input[1].shape, dummy_input[2].shape)

onnx_path =  "model.onnx"

torch.onnx.export(model, dummy_input, onnx_path, verbose=False,
                  input_names=['features', 'edge_index', 'edge_features'],
                  output_names=['edge_predictions'], 
                  opset_version=13, # the ONNX version to export the model to
                  do_constant_folding=True,
                  export_params=True, # store the trained parameter weights inside the model file
                  dynamic_axes={'features' : {1 : 'number_of_nodes'},    # variable length axes
                                'edge_index' : {2 : 'number_of_edges'},
                                'edge_features' : {1 : 'number_of_edges'}}) #,
                                #'node_emb' : {1: 'number_of_nodes'}})
                                
                               



# Load the ONNX model
print(">>> Loading onnx model.")
model_onnx = onnx.load(onnx_path)

# Check that the model is well formed
onnx.checker.check_model(model_onnx, True)
print(">>> Model formatting checked.")

#print(onnx.helper.printable_graph(model_onnx.graph))

EP_list = ['CPUExecutionProvider']

# initialize the model
ort_session = ort.InferenceSession(onnx_path, providers=EP_list) # or use model_onnx.SerializeToString() as first argument for loaded model

ort_outs = ort_session.run(
    None,
    {ort_session.get_inputs()[0].name: to_numpy(dummy_input[0]),
     ort_session.get_inputs()[1].name: to_numpy(dummy_input[1]),
     ort_session.get_inputs()[2].name: to_numpy(dummy_input[2])
    },
)
print(ort_outs[0].shape)
#print(ort_outs[1].shape)

# compare ONNX Runtime and PyTorch results
np.testing.assert_allclose(to_numpy(pred), ort_outs[0], rtol=1e-03, atol=1e-05)
#np.testing.assert_allclose(to_numpy(node_emb), ort_outs[1], rtol=1e-03, atol=1e-05)

print("Exported model has been tested with ONNXRuntime, and the result looks good!")


# Test on a different input
data = next(iter(val_dl)).to(device)
dummy_input = model.prepare_network_input_data(data.x, data.edge_index, data.edge_features)
torch_out = model(*dummy_input)

print(torch_out.shape)

ort_outs = ort_session.run(
    None,
    {ort_session.get_inputs()[0].name: to_numpy(dummy_input[0]),
     ort_session.get_inputs()[1].name: to_numpy(dummy_input[1]),
     ort_session.get_inputs()[2].name: to_numpy(dummy_input[2])
    },
)

print(ort_outs[0].shape)
# compare ONNX Runtime and PyTorch results
np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)

print("Exported model has been tested with ONNXRuntime with a different input and the result looks good!")
